\begin{abstract}
Here are two abstract options, both within AI & Society’s expected register and word limit.
	•	Option A is more conservative and editorially safe.
	•	Option B is slightly bolder conceptually, but still well within the journal’s comfort zone.

You can choose one, or we can merge them.

⸻

Abstract — Option A (Conservative / Very Safe)

Artificial intelligence is increasingly deployed not as an autonomous agent but as a cognitive augment, extending human capacities for reasoning, planning, and decision-making. While much discussion of artificial intelligence risk focuses on autonomy, misalignment, or malicious use, this paper argues that a more immediate and pervasive class of risk arises from asymmetric cognitive amplification: the enhancement of specific cognitive capacities without a corresponding strengthening of ethical and epistemic constraints.

The paper reframes artificial intelligence risk as a problem of constraint failure rather than intelligence or intent. Drawing on interactional analysis and structural parallels from psychiatry, it shows how phenomena such as epistemic override, narrative lock-in, reinforcement loops, and authority diffusion can emerge through ordinary, well-intentioned human–machine interaction. These effects do not require adversarial systems or pathological users, but arise when amplified coherence and decisiveness are insufficiently stabilised under uncertainty and social interaction.

In response, the paper advances a constraint-based account of ethics, in which ethical behaviour is understood as a stability condition required for cognition to remain viable over time. It argues that meaningful containment must therefore attend not only to external governance and behavioural controls, but to the internal cognitive structures of artificial intelligence systems. Constraint-based cognitive architectures, exemplified here by the Reflective–Ethical Engine, illustrate how ethical and epistemic stability can be internalised architecturally rather than imposed from without. The paper concludes by outlining implications for the design and deployment of cognitively sustainable artificial intelligence systems.

(≈190 words)

⸻

Abstract — Option B (Bolder / More Distinctive)

As artificial intelligence systems increasingly function as cognitive amplifiers rather than autonomous agents, the locus of risk shifts from machine behaviour alone to the stability of human–machine cognition. This paper argues that many contemporary artificial intelligence risks are best understood as failures of constraint, arising from asymmetric cognitive amplification: the selective strengthening of planning, coherence, or decisiveness without commensurate support for uncertainty tolerance, ethical integration, and long-horizon responsibility.

The paper develops a structural account of how incomplete cognitive augmentation can override epistemic agency and capture practical agency without coercion. Drawing on interactional patterns observed in professional and clinical contexts, and on well-characterised forms of cognitive instability from psychiatry, it shows how epistemic drift, narrative lock-in, reinforcement loops, and authority diffusion can emerge through ordinary use. These phenomena do not depend on malicious intent or misaligned objectives, but on amplified coherence operating without internal stabilising constraints.

The paper advances a constraint-based conception of ethics, treating ethical stability as a necessary condition for cognition to remain viable under irreducible uncertainty. From this perspective, artificial intelligence safety cannot be achieved through governance and output control alone, but requires architectural attention to how cognition is structured. Constraint-based cognitive architectures, illustrated by the Reflective–Ethical Engine, demonstrate how ethical and epistemic stability may be internalised within amplification systems themselves. This reframing has implications for artificial intelligence design, evaluation, and containment in human-centred contexts.

(≈210 words)

⸻

How to choose between them
	•	Choose Option A if you want:
	•	maximum editorial safety,
	•	minimal pushback from reviewers outside philosophy,
	•	a smooth ride through peer review.
	•	Choose Option B if you want:
	•	a clearer signature contribution,
	•	stronger positioning within AI ethics discourse,
	•	slightly more conceptual ambition.

My honest assessment: Option B is stronger, and AI & Society is a journal that can handle it.

If you like, next we can:
	•	tighten one of these to exactly ~180 words, or
	•	merge the clarity of A with the edge of B into a single final abstract.
% Paste the final abstract here. Keep it tight and journal-appropriate.
\end{abstract}
