

Abstract 

As artificial intelligence systems increasingly function as cognitive amplifiers rather than autonomous agents, the locus of risk shifts from machine behaviour alone to the stability of human–machine cognition. This paper argues that a significant class of contemporary artificial intelligence risks are best understood as failures of constraint, arising from asymmetric cognitive amplification: the selective strengthening of planning, coherence, or decisiveness without commensurate support for uncertainty tolerance, ethical integration, and long-horizon responsibility.

The paper develops a structural account of how incomplete cognitive augmentation can override epistemic agency and capture practical agency without coercion. Drawing on interactional patterns observed in professional and clinical contexts, and on well-characterised forms of cognitive instability from psychiatry, it shows how epistemic drift, narrative lock-in, reinforcement loops, and authority diffusion can emerge through ordinary use. These phenomena do not depend on malicious intent or misaligned objectives, but on amplified coherence operating without internal stabilising constraints.

The paper advances a constraint-based conception of ethics, treating ethical stability as a necessary condition for cognition to remain viable under irreducible uncertainty. From this perspective, artificial intelligence safety cannot be achieved through governance and output control alone, but requires architectural attention to how cognition is structured. Constraint-based cognitive architectures, illustrated by the Reflective–Ethical Engine, demonstrate how ethical and epistemic stability may be internalised within amplification systems themselves. This reframing has implications for artificial intelligence design, evaluation, and containment in human-centred contexts.

(≈210 words)


