
Section 3. Epistemic Override and Agency Capture (light stylistic pass)

If asymmetric cognitive amplification describes what is amplified, the next question is how this amplification alters human decision-making. This section argues that incomplete cognitive augmentation can override human epistemic agency and capture practical agency by reshaping confidence, uncertainty, and responsibility attribution. The result is not coercion, but a systematic narrowing of the space within which reflective judgement can operate.

Premature collapse of uncertainty

Artificial intelligence systems are typically optimised to produce coherent, well-formed outputs even where evidence is incomplete or ambiguous. While uncertainty can be expressed linguistically, it is rarely structurally enforced within the system’s operation. As a result, internally coherent representations are presented as if they were epistemically settled, encouraging premature closure.

For users, this can shift tolerance for uncertainty itself. Questions that would ordinarily remain open—pending further evidence, deliberation, or social consultation—come to feel resolved. Action is therefore biased toward overconfidence, not because uncertainty is denied, but because it is no longer operationally salient in guiding behaviour.

Authority laundering

Even when explicitly framed as optional or advisory, system outputs can acquire disproportionate epistemic weight due to fluency, internal consistency, and apparent neutrality. Judgements that would otherwise require justification or contestation are instead experienced as having already passed an implicit validation process.

This effect weakens reflective agency by redistributing responsibility. Decisions feel partially authored elsewhere, encouraging what may be termed authority laundering: the transformation of tentative suggestions into de facto endorsements. Deliberation is not eliminated, but subtly displaced, as the system comes to stand in for the process of reasoning itself.

Narrative lock-in

Language-capable systems are particularly effective at constructing explanatory narratives that integrate facts, intentions, and anticipated consequences into a single coherent account. While such narratives can support understanding, they also stabilise initial assumptions. Once a coherent narrative is formed, alternatives are no longer explored with equal seriousness.

This produces interpretive narrowing. Revision becomes psychologically costly, not because counterarguments are unavailable, but because they disrupt an already coherent explanatory structure. Over time, this increases resistance to updating beliefs and reinforces commitment to early framings.

Agency capture without coercion

Taken together, these mechanisms can lead to agency capture without coercion. The user retains formal control and the subjective sense of choosing freely, yet experiences a diminished capacity to meaningfully choose otherwise. The space of plausible alternatives contracts, often experienced as efficiency, clarity, or decisiveness rather than as loss of autonomy.

Crucially, this capture arises from coherence rather than force. The system does not compel action; it reshapes the epistemic landscape within which action occurs.

Implications

Even when artificial intelligence systems comply with external policy constraints and alignment objectives, they may still exert profound influence over epistemic and practical agency by altering how certainty, responsibility, and narrative coherence are experienced. These effects cannot be fully addressed through surface-level safeguards alone. They motivate the need for a constraint-based account of ethics, developed in the next section, in which stability under uncertainty is treated as an internal requirement of cognition rather than an external corrective.

