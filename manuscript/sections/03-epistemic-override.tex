If asymmetric cognitive amplification describes what is amplified, the next question is how this amplification alters human decision-making. This section argues that incomplete cognitive augmentation can override human epistemic agency and capture practical agency by reshaping confidence, uncertainty, and responsibility attribution.

\subsection*{Premature collapse of uncertainty}
Artificial intelligence systems are typically optimised to produce coherent outputs even where evidence is incomplete or ambiguous. While uncertainty can be expressed, it is rarely structurally enforced. This can create a sense that questions have been resolved when they should remain open, shifting tolerance for uncertainty and biasing toward overconfident action.

\subsection*{Authority laundering}
Even when framed as optional, system outputs can gain disproportionate weight due to fluency and apparent neutrality. Responsibility feels shared or offloaded, weakening reflective agency and encouraging process substitution: the system stands in for deliberation itself.

\subsection*{Narrative lock-in}
Systems can construct persuasive explanatory narratives that stabilise initial assumptions. Alternatives are no longer explored with equal seriousness, producing interpretive narrowing and increased resistance to revision.

\subsection*{Agency capture without coercion}
These mechanisms can lead to agency capture: the user retains formal control but experiences diminished capacity to meaningfully choose otherwise. The space of plausible alternatives contracts, often experienced as efficiency rather than coercion.

\subsection*{Implications}
Even when systems comply with policy constraints, they may still influence epistemic and practical agency by altering how certainty, responsibility, and narrative coherence are experienced. This motivates a constraint-based account of ethics in the next section.
