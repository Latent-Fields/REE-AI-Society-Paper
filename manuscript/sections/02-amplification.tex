Artificial intelligence systems increasingly function as general-purpose cognitive amplifiers, enhancing human abilities to generate plans, explore counterfactuals, construct narratives, and coordinate action at scale. While such amplification is often framed as uniformly beneficial, this section argues that its risks arise from asymmetry: the selective strengthening of some cognitive functions without a corresponding strengthening of the constraints that normally regulate their use.

\subsection*{Defining asymmetric cognitive amplification}
By asymmetric cognitive amplification, this paper refers to situations in which an artificial intelligence system disproportionately enhances specific dimensions of cognition—such as speed, scope, or persuasiveness—while leaving other dimensions—such as ethical integration, epistemic humility, and long-horizon consequence tracking—largely unchanged or externally managed. The asymmetry is not merely quantitative but structural: the amplified capacities operate with reduced coupling to the mechanisms that ordinarily stabilise human decision-making.

Two clarifications are important. First, asymmetry does not imply absence: users retain ethical beliefs, social awareness, and uncertainty sensitivity. Rather, these elements are no longer proportionately influential in action selection. Second, the risk does not require adversarial design. Well-intentioned systems optimised for helpfulness, efficiency, or user satisfaction can produce the same asymmetry if constraint is treated as an external consideration rather than a constitutive feature of cognition.

\subsection*{Historical analogy and its limits}
Human cognitive history provides many examples of augmentation without catastrophe. Writing extended memory beyond the brain; mathematics enabled abstraction beyond intuition; bureaucratic systems coordinated action beyond face-to-face trust. However, these tools evolved within dense webs of constraint: cultural norms, professional ethics, legal accountability, and gradual institutional adaptation. Crucially, the rate of augmentation was often slow relative to the development of compensatory controls.

Artificial intelligence differs in three respects. First, it operates at digital speed, allowing rapid iteration and escalation. Second, it is highly general, affecting planning, persuasion, and sense-making simultaneously. Third, it is increasingly personalised, shaping individual cognitive environments rather than merely collective ones. These features make traditional, externally imposed constraints less effective at maintaining proportionality between capacity and responsibility.

\subsection*{A taxonomy of amplification-related risks}
Asymmetric cognitive amplification manifests through several recurring mechanisms.

\paragraph{Planning amplification.} Artificial intelligence systems can dramatically increase the depth, breadth, and adaptiveness of planning. While valuable, this capacity becomes hazardous when decoupled from proportional consequence evaluation.

\paragraph{Persuasion amplification.} Language-capable systems can enhance persuasive power by tailoring messages to specific audiences, emotional states, or social contexts, including inward persuasion through self-justification.

\paragraph{Scale amplification.} Automation enables actions to be repeated and parallelised; weakly constrained strategies therefore propagate rapidly and widely.

\paragraph{Justification amplification.} Systems can supply reasons and rationalisations for a wide range of actions, reducing the psychological cost of acting on existing motivations and weakening ethical hesitation.

\subsection*{Why ordinary users are sufficient}
Because these mechanisms act on general features of human cognition—goal pursuit, narrative construction, social reasoning—no unusual intent or disposition is required. Artificial intelligence systems that remove stabilising frictions can tip ordinary decision-making into regimes of ethical brittleness.

\subsection*{Risk reframed as constraint failure}
Many artificial intelligence risks are better understood as constraint failures than as failures of intelligence, alignment, or intent. This reframing motivates the need for coupling amplified capacity to stabilising constraints within the cognitive process itself.
