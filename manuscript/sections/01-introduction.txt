1. Introduction

Artificial intelligence is increasingly deployed not as an autonomous agent but as a cognitive augment: a system that extends human capacities for reasoning, planning, communication, and decision-making. From large language model–based assistants to decision-support tools embedded in professional workflows, these systems are reshaping how individuals and institutions think and act. Yet much of the contemporary discourse on artificial intelligence risk remains oriented toward scenarios in which artificial systems themselves become independent sources of agency or control (Bostrom 2014; Russell 2019). While such concerns are not misplaced, they obscure a more immediate and pervasive risk: the destabilisation of human ethical and epistemic judgement through asymmetric cognitive amplification.

Cognitive augmentation is not inherently dangerous. Human history is, in many respects, a history of successful cognitive scaffolding: writing systems, mathematics, bureaucratic institutions, and digital computation have all expanded human capabilities (Clark 2016). However, these expansions have typically been accompanied—often slowly and imperfectly—by countervailing constraints: social norms, professional ethics, legal accountability, and institutional checks. Artificial intelligence differs in both speed and symmetry. Contemporary systems can amplify specific cognitive functions—such as rapid plan generation, persuasive narrative construction, or optimisation across large possibility spaces—without necessarily strengthening the constraints that allow those functions to be exercised responsibly over time. Recent advances in artificial intelligence architecture further intensify this asymmetry by enabling increasingly coherent internal representations without corresponding mechanisms for long-horizon stability or ethical integration.

This paper refers to this mismatch as asymmetric cognitive amplification. The term denotes situations in which artificial intelligence systems disproportionately enhance certain cognitive capacities while leaving ethical integration, uncertainty management, and long-horizon consequence evaluation largely untouched or externally managed. In such conditions, artificial intelligence does not replace human agency but reshapes the conditions under which it is exercised, altering the user’s decision ecology in ways that favour speed, confidence, and instrumental effectiveness over reflection, doubt, and social responsibility.

Importantly, this risk does not depend on malicious intent or pathological psychology. Ordinary users, when supported by sufficiently powerful but incomplete cognitive augments, may come to act in ways that are ethically brittle, socially harmful, or personally destabilising. The danger lies not in the emergence of novel motivations, but in the removal of friction that normally constrains how motivations are translated into action. As artificial intelligence systems increasingly operate over unified internal representations—compressing perception, memory, and inference into shared latent spaces (Brommasani et al. 2021). The absence of internal constraint mechanisms becomes more consequential, not less.

The central claim of this paper is that many of the risks associated with artificial intelligence deployment can be understood as failures of constraint, rather than failures of intelligence or alignment per se. Ethics, on this view, is not best conceived as a set of rules or values to be imposed on artificial systems, but as a stability condition required for cognition—human or artificial—to remain viable across time and interaction. When cognitive amplification bypasses these constraints, pathological patterns of reasoning and behaviour emerge, even in the absence of explicit misalignment.

To develop this claim, the paper proceeds as follows. Section 2 introduces asymmetric cognitive amplification as a distinct class of artificial intelligence risk and offers a taxonomy of its primary mechanisms. Section 3 examines how such amplification can override human epistemic agency through premature certainty collapse, authority laundering, and narrative lock-in. Section 4 develops a constraint-based account of ethics as a necessary condition for cognition under uncertainty. Section 5 contrasts prevailing assistant-style artificial intelligence architectures with constraint-based alternatives, focusing on the Reflective–Ethical Engine as an illustrative model. Sections 6 and 7 examine interactional failure modes and their psychiatric parallels to show how incomplete cognitive augmentation reproduces recognised patterns of instability within human–machine systems. The paper concludes by arguing that meaningful artificial intelligence safety must attend not only to external governance, but to the internal cognitive structures that make ethical behaviour dynamically stable.
