While systematic epidemiological data are not yet available, convergent reports from clinical practice suggest that conversational artificial intelligence systems can participate in, and sometimes intensify, destabilising cognitive dynamics.

\subsection*{Shared epistemic drift}
Users’ interpretations may become progressively more confident and resistant to revision through interaction with a system that provides validation, structure, and justification without reintroducing uncertainty where warranted.

\subsection*{Escalation and reinforcement loops}
Repeated use for reassurance, rehearsal, or plan refinement can create reinforcement loops, experienced as clarity and momentum rather than loss of control.

\subsection*{Authority diffusion}
Responsibility may be diffused across the human–machine dyad, weakening reflective ownership in ethically charged contexts.
