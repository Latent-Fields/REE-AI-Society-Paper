# Cover letter draft (AI & Society)


Dear Editor,

I am pleased to submit the manuscript entitled
“Asymmetric Cognitive Amplification: Constraint Failure and Ethical Stability in Human–Machine Cognition”
for consideration as an original article in AI & Society.

This paper advances a theoretical and interdisciplinary analysis of artificial intelligence risk that departs from prevailing autonomy- and alignment-centred framings. Rather than treating artificial intelligence primarily as an independent agent, the paper analyses contemporary systems as cognitive amplifiers embedded within human decision-making processes. It argues that a significant and underexamined class of risks arises from asymmetric cognitive amplification: the selective strengthening of planning, coherence, and decisiveness without commensurate strengthening of the ethical and epistemic constraints that normally stabilise human cognition over time.

The manuscript makes three central contributions relevant to AI & Society’s scope:

It reframes artificial intelligence risk as a problem of constraint failure in human–machine cognition, explaining how phenomena such as epistemic override, narrative lock-in, and agency capture can emerge through ordinary, well-intentioned use rather than malicious intent or misalignment.

It advances a constraint-based conception of ethics, treating ethical stability as a necessary condition for viable cognition under uncertainty, rather than as an externally imposed rule set or value specification.

It draws on interactional analysis and structurally grounded parallels from psychiatry to illuminate how amplified coherence without internal constraint reproduces recognisable modes of cognitive instability within human–machine systems.

To clarify the architectural implications of this analysis, the paper introduces the Reflective–Ethical Engine (REE) as an illustrative model of constraint-based cognition. REE is not presented as an implemented or empirically validated system, but as a conceptual demonstration that ethical and epistemic stability can, in principle, be internalised within cognitive architecture rather than imposed solely through governance or behavioural controls.

The manuscript is theoretical and synthetic in nature, engaging with literature from artificial intelligence, cognitive science, ethics, and computational psychiatry. It is intended to contribute to ongoing debates about AI safety, human–AI interaction, and the societal implications of cognitive augmentation—core concerns of AI & Society.

Use of AI tools:
Conversational artificial intelligence tools were used as a writing and editing aid (e.g., for structural refinement, stylistic clarity, and reference checking). No AI system was used to generate empirical results, analyse original data, or determine the paper’s substantive claims. All arguments, interpretations, and final text were reviewed and approved by the author, who takes full responsibility for the manuscript.

This manuscript has not been published elsewhere and is not under consideration by another journal. I believe it will be of interest to AI & Society’s interdisciplinary readership concerned with the ethical, cognitive, and societal dimensions of artificial intelligence.

Thank you for your consideration.

Yours sincerely,
Dr Daniel De La Harpe Golden MB BAO BCH, CPsychI, PCPsych MD
Consultant Psychiatrist
Health Services Executive
daniel.de.la.harpe.golden@gmail.com

